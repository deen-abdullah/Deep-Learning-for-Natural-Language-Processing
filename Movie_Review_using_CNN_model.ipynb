{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Review using CNN model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpuruo10ZH0TLsWzbU+aPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabimist/Deep-Learning-for-Natural-Language-Processing/blob/main/Movie_Review_using_CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPh-Iz4mp0vQ"
      },
      "source": [
        "**Name:** Deen Mohammad Abdullah\n",
        "\n",
        "The purpose of this project is to train and test Movie Review dataset of NLTK using CNN.\n",
        "\n",
        "**Deep Learning for Natural Language Processing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfbH8hKqO2n"
      },
      "source": [
        "Execute the following code to run the project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq54AevBpmgq",
        "outputId": "83735090-064c-4421-bbc0-ee3615c41d27"
      },
      "source": [
        "############################################ Required Packages #####################################################################\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('movie_reviews') # -------------------------------- downloading the movie_review dataset\n",
        "nltk.download('stopwords') # ------------------------------------ downloading the stopwords from NLTK\n",
        "nltk.download('wordnet')  # ------------------------------------- for lematization we need to download wordnet (WordNetLemmatizer)\n",
        "nltk.download('punkt') # ---------------------------------------- used for word tokenization\n",
        "from nltk.corpus import movie_reviews # ------------------------- importing the movie_review dataset\n",
        "from nltk.corpus import stopwords # ----------------------------- importing the stopwords\n",
        "from nltk.stem import WordNetLemmatizer # ----------------------- importing wordNetLemmatizer for lemmatization\n",
        "from nltk.tokenize import word_tokenize # ----------------------- tokenize words from text\n",
        "import re # ----------------------------------------------------- importing regularExpression to extract only text from the dataset\n",
        "from sklearn.model_selection import train_test_split #----------- split the dataset for training and validation set\n",
        "import torch\n",
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import random\n",
        "######################################################################################################################################\n",
        "\n",
        "########################################### Function Definitions (I have defined nine functions) #####################################\n",
        "#------ (1) This function removes special charecters from text ---------\n",
        "def removeSpecialCharacter(word_list):\n",
        "  cleanWordList = []\n",
        "  \n",
        "  for word in word_list:\n",
        "    if (re.match('[a-zA-Z0-9]+', word)):\n",
        "      cleanWordList.append(word.lower())\n",
        "  \n",
        "  return cleanWordList\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "#------ (2) This function removes stop words from text -----------------\n",
        "def removeStopWords (word_list):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "  filteredWords = [] \n",
        "  \n",
        "  for word in word_list:\n",
        "    if word not in stop_words: \n",
        "      filteredWords.append(word)\n",
        "      \n",
        "  return filteredWords\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "#------ (3) This function uses WordNet and lematizes the text -----------\n",
        "def lemmatize (word_list):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  filteredWords = []\n",
        "  \n",
        "  for word in word_list:\n",
        "    filteredWords.append(lemmatizer.lemmatize(word))\n",
        "    \n",
        "  return filteredWords\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "#--- (4) This function tokenize text, build vocabulary, preparing input  \n",
        "#----------- for cnn layer and calculate max length of text -------------\n",
        "def extractFeature(texts):\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "\n",
        "    word2idx['<pad>'] = 0\n",
        "    \n",
        "    idx = 1\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx = idx + 1\n",
        "\n",
        "        if len(tokenized_sent) > max_len:\n",
        "          max_len = len(tokenized_sent)\n",
        "\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "      tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "      input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "      input_ids.append(input_id)\n",
        "    input_ids = np.array(input_ids)\n",
        "\n",
        "    return tokenized_texts, word2idx, input_ids, max_len\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "# ---- (5) This function loads fastText as pretrained vector ----------\n",
        "#------------- and prepares input embedding ---------------------------\n",
        "def pretrained_vectors(word2idx):\n",
        "  URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "  FILE = \"fastText\"\n",
        "  \n",
        "  if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "  else:\n",
        "    print ('fastText pretrained vector does not exist. It will take time (few minutes) to download it ...')\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE\n",
        "\n",
        "  fname = \"fastText/crawl-300d-2M.vec\"\n",
        "  print (\"Using pretrained vectors for embedding. It will take some minutes. Please wait...\")\n",
        "  fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  n, d = map(int, fin.readline().split())\n",
        "  \n",
        "  embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "  embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "  \n",
        "  count = 0\n",
        "  for line in fin:\n",
        "    tokens = line.rstrip().split(' ')\n",
        "    word = tokens[0]\n",
        "    if word in word2idx:\n",
        "      count += 1\n",
        "      embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "  \n",
        "  embeddings = torch.tensor(embeddings)\n",
        "  print(\"Done Embedding\")\n",
        "  return embeddings\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "# ---- (6) Purpose of this function is to convert training ----------------------\n",
        "#---------- and validation dataset into torchTensor -----------------------------\n",
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels):\n",
        "    train_inputs, val_inputs, train_labels, val_labels = tuple(torch.tensor(data) for data in [train_inputs, val_inputs, train_labels, val_labels])\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = 50)\n",
        "\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = 50)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "########### CNN Model Architecture starts from here #############################\n",
        "class CNN_MODEL(nn.Module):\n",
        "  def __init__(self, pretrained_embedding = None, freeze_embedding = False, vocab_size = None, embed_dim = 300, filter_sizes = [3, 4, 5], num_filters = [100, 100, 100], num_classes = 2, dropout = 0.5):\n",
        "    super(CNN_MODEL, self).__init__()\n",
        "    \n",
        "    # Initializes input embedding layer\n",
        "    self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "    self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze = freeze_embedding)\n",
        "\n",
        "    # Initializes convolution Layer    \n",
        "    self.conv1d_list = nn.ModuleList([nn.Conv1d(in_channels = self.embed_dim, out_channels = num_filters[i], kernel_size = filter_sizes[i]) for i in range(len(filter_sizes))])\n",
        "    \n",
        "    # Initializes fully connected layer\n",
        "    self.fc = nn.Linear(np.sum(num_filters), num_classes) # there are two classes (pos/neg) in movie review\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self, input_ids):\n",
        "    # Embedding from input_ids\n",
        "    x_embed = self.embedding(input_ids).float()\n",
        "    \n",
        "    # Preparing the input for convolution layer\n",
        "    x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "    # Covolution Layer and ReLU layer    \n",
        "    x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "    \n",
        "    # Applied Max pooling\n",
        "    x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "    \n",
        "    # Fully connected layer (Concatenated all the output from the pooling layer and put it to the fully conected layer)\n",
        "    x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
        "    logits = self.fc(self.dropout(x_fc))\n",
        "    \n",
        "    return logits\n",
        "\n",
        "# -------- (7) This function is initializing the CNN model ----------------------------\n",
        "def initilize_model(pretrained_embedding):\n",
        "  filter_sizes=[3, 4, 5]\n",
        "  num_filters=[100, 100, 100]\n",
        "  learning_rate=0.1\n",
        "  \n",
        "  assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and num_filters need to be of the same length.\"\n",
        "  \n",
        "  # Here cnn_model is the instance of CNN_MODEL Class\n",
        "  cnn_model = CNN_MODEL(pretrained_embedding = pretrained_embedding, freeze_embedding = True, vocab_size = None, embed_dim = 300, filter_sizes = filter_sizes, num_filters = num_filters, num_classes=2, dropout = 0.5)\n",
        "  cnn_model.to(device)\n",
        "  \n",
        "  optimizer = optim.Adadelta (cnn_model.parameters(), lr=learning_rate, rho=0.95)\n",
        "  \n",
        "  return cnn_model, optimizer\n",
        "#--------------------------------------------------------------------------------\n",
        "############ CNN Model architecture ends here #####################################\n",
        "\n",
        "###################### Training and Evaluation of Model ###########################\n",
        "#---- (8) this function evaluates the model -------------------------------------\n",
        "def evaluate(model, val_dataloader):\n",
        "  model.eval()\n",
        "  val_accuracy = []\n",
        "  val_loss = []\n",
        "  \n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids)\n",
        "    \n",
        "    loss = loss_fn(logits, b_labels)\n",
        "    val_loss.append(loss.item())\n",
        "    preds = torch.argmax(logits, dim=1).flatten()\n",
        "    \n",
        "    accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "    val_accuracy.append(accuracy)\n",
        "  \n",
        "  val_loss = np.mean(val_loss)\n",
        "  val_accuracy = np.mean(val_accuracy)\n",
        "  \n",
        "  return val_loss, val_accuracy\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#------ (9) this function train the model -------------------------------------- \n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "def train(model, optimizer, train_dataloader, val_dataloader, epochs):\n",
        "  best_accuracy = 0\n",
        "  \n",
        "  print(\"Training starts...\")\n",
        "  \n",
        "  for epoch_i in range(epochs):\n",
        "    total_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "      model.zero_grad() # initializing all gradients with zero value\n",
        "      logits = model(b_input_ids)\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "    if val_accuracy > best_accuracy:\n",
        "      best_accuracy = val_accuracy\n",
        "    print (\"Epoch: \" + str (epoch_i + 1))\n",
        "    print (\"Training Loss: \" + str (\"{:.2f}\".format(avg_train_loss)) + \", Validation Loss: \" + str (\"{:.2f}\".format(val_loss)))\n",
        "    print (\"Model Accuracy: \" + str (val_accuracy) + \"%\")\n",
        "    print (\"-------------------------------------------------------------------\")\n",
        "  print (\"\\n\")\n",
        "  print (\"Training is Done\")\n",
        "  print (\"Best Model Accuracy: \" + str(\"{:.2f}\".format(best_accuracy)) + \"%\")\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "###############            Main Function              #########################\n",
        "############### Executable statements start from here #########################\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "document = [(movie_reviews.words(file_id),category) for file_id in movie_reviews.fileids() for category in movie_reviews.categories(file_id)]\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for (word_list,category) in document:\n",
        "  word_list = removeSpecialCharacter (word_list)\n",
        "  word_list = removeStopWords (word_list)\n",
        "  word_list = lemmatize (word_list)\n",
        "  \n",
        "  txt = ''\n",
        "  for w in word_list:\n",
        "    txt = txt + w + ' '\n",
        "    \n",
        "  texts.append(txt)\n",
        "  \n",
        "  if category == 'neg':\n",
        "    labels.append(0)\n",
        "  else:\n",
        "    labels.append(1)\n",
        "\n",
        "tokenized_texts, word2idx, input_ids, max_len = extractFeature(texts)\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1)\n",
        "train_dataloader, val_dataloader = data_loader(train_inputs, val_inputs, train_labels, val_labels)\n",
        "embeddings = pretrained_vectors(word2idx)\n",
        "cnn_model, optimizer = initilize_model (pretrained_embedding=embeddings)\n",
        "train(cnn_model, optimizer, train_dataloader, val_dataloader, epochs=25)\n",
        "\n",
        "######################## End of Main function ##################################\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "fastText exists.\n",
            "Using pretrained vectors for embedding. It will take some minutes. Please wait...\n",
            "Done Embedding\n",
            "Training starts...\n",
            "Epoch: 1\n",
            "Training Loss: 0.71, Validation Loss: 0.69\n",
            "Model Accuracy: 47.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 2\n",
            "Training Loss: 0.68, Validation Loss: 0.67\n",
            "Model Accuracy: 73.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 3\n",
            "Training Loss: 0.66, Validation Loss: 0.67\n",
            "Model Accuracy: 69.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 4\n",
            "Training Loss: 0.64, Validation Loss: 0.66\n",
            "Model Accuracy: 60.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 5\n",
            "Training Loss: 0.61, Validation Loss: 0.64\n",
            "Model Accuracy: 84.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 6\n",
            "Training Loss: 0.59, Validation Loss: 0.63\n",
            "Model Accuracy: 83.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 7\n",
            "Training Loss: 0.57, Validation Loss: 0.61\n",
            "Model Accuracy: 80.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 8\n",
            "Training Loss: 0.53, Validation Loss: 0.59\n",
            "Model Accuracy: 78.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 9\n",
            "Training Loss: 0.50, Validation Loss: 0.56\n",
            "Model Accuracy: 86.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 10\n",
            "Training Loss: 0.48, Validation Loss: 0.55\n",
            "Model Accuracy: 83.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 11\n",
            "Training Loss: 0.44, Validation Loss: 0.52\n",
            "Model Accuracy: 87.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 12\n",
            "Training Loss: 0.41, Validation Loss: 0.52\n",
            "Model Accuracy: 82.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 13\n",
            "Training Loss: 0.38, Validation Loss: 0.49\n",
            "Model Accuracy: 87.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 14\n",
            "Training Loss: 0.35, Validation Loss: 0.47\n",
            "Model Accuracy: 87.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 15\n",
            "Training Loss: 0.33, Validation Loss: 0.46\n",
            "Model Accuracy: 86.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 16\n",
            "Training Loss: 0.30, Validation Loss: 0.46\n",
            "Model Accuracy: 83.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 17\n",
            "Training Loss: 0.27, Validation Loss: 0.43\n",
            "Model Accuracy: 86.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 18\n",
            "Training Loss: 0.25, Validation Loss: 0.42\n",
            "Model Accuracy: 85.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 19\n",
            "Training Loss: 0.23, Validation Loss: 0.41\n",
            "Model Accuracy: 85.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 20\n",
            "Training Loss: 0.21, Validation Loss: 0.40\n",
            "Model Accuracy: 86.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 21\n",
            "Training Loss: 0.19, Validation Loss: 0.39\n",
            "Model Accuracy: 86.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 22\n",
            "Training Loss: 0.16, Validation Loss: 0.39\n",
            "Model Accuracy: 86.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 23\n",
            "Training Loss: 0.15, Validation Loss: 0.38\n",
            "Model Accuracy: 86.5%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 24\n",
            "Training Loss: 0.14, Validation Loss: 0.37\n",
            "Model Accuracy: 87.0%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 25\n",
            "Training Loss: 0.13, Validation Loss: 0.37\n",
            "Model Accuracy: 86.0%\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training is Done\n",
            "Best Model Accuracy: 87.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}