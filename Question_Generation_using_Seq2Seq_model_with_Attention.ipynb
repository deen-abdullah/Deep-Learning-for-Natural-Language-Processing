{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question Generation using Seq2Seq model with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EtftSOSKyIFkM8C6LtMTM4eDhXObtk59",
      "authorship_tag": "ABX9TyMxocZM3jd5Wf2Z1WYulEIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c67ff5e62df4d1d880298ca5515c73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ab08657d67a641b9b884916d5f4188cc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d15a9d6d8bdd48898fd9e047be4074e8",
              "IPY_MODEL_bdc7fcf862bb45f2ac0794d858fde5c9"
            ]
          }
        },
        "ab08657d67a641b9b884916d5f4188cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d15a9d6d8bdd48898fd9e047be4074e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34c390cd98f74acf9e1b0af87d7b9331",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b93e6c36ce7544feac1436baee482ea7"
          }
        },
        "bdc7fcf862bb45f2ac0794d858fde5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c04ce88561e454aa9352bb77bb1026c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000000/1000000 [6:15:27&lt;00:00, 44.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3eae016ea6444efbb0a04cf8e4d7a8a9"
          }
        },
        "34c390cd98f74acf9e1b0af87d7b9331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b93e6c36ce7544feac1436baee482ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c04ce88561e454aa9352bb77bb1026c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3eae016ea6444efbb0a04cf8e4d7a8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabimist/Deep-Learning-for-Natural-Language-Processing/blob/main/Question_Generation_using_Seq2Seq_model_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enK1z_od2hA"
      },
      "source": [
        "**Name:** Deen Mohammad Abdullah\n",
        "\n",
        "Question Generation using Seq2Seq model with Attention.\n",
        "\n",
        "**Dataset:** qa_Health_and_Personal_Care ([Amazon](https://jmcauley.ucsd.edu/data/amazon/qa/))\n",
        "\n",
        "**Deep Learning for Natural Language Processing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJp5Ra53a2hF"
      },
      "source": [
        "**The reason behind the higher loss value:**\n",
        "\n",
        "Here, I have calculated the loss by considering same positions of prediction and target.\n",
        "\n",
        "Example:  loss = criteria (prediction[i], target[i])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c67ff5e62df4d1d880298ca5515c73c",
            "ab08657d67a641b9b884916d5f4188cc",
            "d15a9d6d8bdd48898fd9e047be4074e8",
            "bdc7fcf862bb45f2ac0794d858fde5c9",
            "34c390cd98f74acf9e1b0af87d7b9331",
            "b93e6c36ce7544feac1436baee482ea7",
            "9c04ce88561e454aa9352bb77bb1026c",
            "3eae016ea6444efbb0a04cf8e4d7a8a9"
          ]
        },
        "id": "2-c_U75k709F",
        "outputId": "63c4e3f3-7a3c-47d8-9222-eae309dae74d"
      },
      "source": [
        "############################################ Required Packages #####################################################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import gzip\n",
        "from tqdm.notebook import tqdm\n",
        "######################################################################################################################################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "############################# Text Processing ##################################\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 20\n",
        "MIN_LENGTH = 5\n",
        "\n",
        "# This class helps us to process the source and target documents\n",
        "class Text:\n",
        "   def __init__(self):\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "       self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1\n",
        "#-----------------------------------------------------------------------\n",
        "def normalize_sentence(df, index):\n",
        "   sentence = df[index].str.lower() \n",
        "   sentence = sentence.str.replace('[^A-Za-z\\s]+', '')\n",
        "   sentence = sentence.str.normalize('NFD')\n",
        "   sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "   return sentence\n",
        "#------------------------------------------------------------------------\n",
        "def read_sentence(df, src, tgt):\n",
        "   print ('Processing Text . . . ')\n",
        "   sentence1 = normalize_sentence(df, src)\n",
        "   sentence2 = normalize_sentence(df, tgt)\n",
        "   return sentence1, sentence2\n",
        "#------------------------------------------------------------------------\n",
        "def parse(path): \n",
        "  g = gzip.open(path, 'rb') \n",
        "  \n",
        "  for l in g: \n",
        "    yield eval(l) \n",
        "#------------------------------------------------------------------------\n",
        "def getDF(path): \n",
        "  i = 0 \n",
        "  df = {} \n",
        "  for d in parse(path): \n",
        "    df[i] = d \n",
        "    i += 1 \n",
        "  \n",
        "  return pd.DataFrame.from_dict(df, orient='index') \n",
        "#------------------------------------------------------------------------  \n",
        "def process_data(src,tgt):\n",
        "  df = getDF('/content/drive/MyDrive/qa_Health_and_Personal_Care.json.gz')\n",
        "  df1 = getDF('/content/drive/MyDrive/qa_Home_and_Kitchen.json.gz')\n",
        "  print('Before Processing Data, the number of records: %d' % (len(df)+len(df1)))\n",
        "  print ('File reading complete')\n",
        "  sentence1, sentence2 = read_sentence(df, src, tgt)\n",
        "  sentence11, sentence22 = read_sentence(df1, src, tgt)\n",
        "  \n",
        "  source = Text()\n",
        "  target = Text()\n",
        "  pairs = []\n",
        "  count = 0\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH and len(sentence1[i].split(' ')) > MIN_LENGTH:\n",
        "      full = [sentence1[i], sentence2[i]]\n",
        "      source.addSentence(sentence1[i])\n",
        "      target.addSentence(sentence2[i])\n",
        "      pairs.append(full)\n",
        "      count = count + 1\n",
        "\n",
        "  for i in range(len(df1)):\n",
        "    if len(sentence11[i].split(' ')) < MAX_LENGTH and len(sentence22[i].split(' ')) < MAX_LENGTH and len(sentence11[i].split(' ')) > MIN_LENGTH:\n",
        "      full = [sentence11[i], sentence22[i]]\n",
        "      source.addSentence(sentence11[i])\n",
        "      target.addSentence(sentence22[i])\n",
        "      pairs.append(full)\n",
        "      count = count + 1\n",
        "  \n",
        "  print ('Processing Complete')\n",
        "  print ('After Processing Data, the number of records: %d' % count)\n",
        "  \n",
        "  return source, target, pairs\n",
        "#------------------------------------------------------------------------\n",
        "def indexesFromSentence(lang, sentence):\n",
        "   return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "#------------------------------------------------------------------------\n",
        "def tensorFromSentence(lang, sentence):\n",
        "   indexes = indexesFromSentence(lang, sentence)\n",
        "   indexes.append(EOS_token)\n",
        "   return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "#------------------------------------------------------------------------\n",
        "def tensorsFromPair(input_text, output_text, pair):\n",
        "   input_tensor = tensorFromSentence(input_text, pair[0])\n",
        "   target_tensor = tensorFromSentence(output_text, pair[1])\n",
        "   return (input_tensor, target_tensor)\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### Our Model ######################################\n",
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "       super(Encoder, self).__init__()\n",
        "      \n",
        "       self.hidden_size = hidden_size\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "       self.dropout = nn.Dropout(p)\n",
        "       self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "       self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional = True, dropout=p)\n",
        "       \n",
        "       # For dimension adjustment of bidirectional LSTM\n",
        "       self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
        "       self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
        "              \n",
        "   def forward(self, x):\n",
        "      \n",
        "       #embedded = self.embedding(src).view(1,1,-1)\n",
        "       embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "       encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "       hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "       cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "       #outputs, hidden = self.gru(embedded)\n",
        "       return encoder_states, hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       self.hidden_size = hidden_size       \n",
        "       self.num_layers = num_layers\n",
        "       self.output_size = output_size\n",
        "\n",
        "       self.dropout = nn.Dropout(p)\n",
        "\n",
        "       self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "       self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "       self.energy = nn.Linear(hidden_size*3, 1)\n",
        "       self.softmax = nn.Softmax(dim=0)\n",
        "       self.relu = nn.ReLU()\n",
        "       self.fc = nn.Linear(hidden_size, output_size)\n",
        "       \n",
        "   def forward(self, x, encoder_states, hidden, cell):\n",
        "\n",
        "# reshape the x to (1, batch_size) as we are predicting one word at a time\n",
        "       x = x.unsqueeze(0)\n",
        "       embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "       sequence_length = encoder_states.shape[0]\n",
        "       h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "       energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim = 2)))\n",
        "       attention = self.softmax(energy)                  # [sequence_len, batch_size, 1]\n",
        "       attention = attention.permute(1, 2, 0)            # [batch_size, 1, sequence_len]\n",
        "       encoder_states = encoder_states.permute(1, 0, 2)  # [batch_size, sequence_len, hidden_size*2]\n",
        "\n",
        "       #matrix multiplication to calculate the context vector\n",
        "       context_vector = torch.bmm(attention, encoder_states).permute(1, 0, 2)  # [batch_size, 1, hidden_size*2] to [1, batch_size, hidden_size*2]\n",
        "\n",
        "       # for each time-step during predicting each word, we are concatenating this context vector to maintain the attention\n",
        "       # instead of embedding, this rnn_input will be our input\n",
        "       rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "\n",
        "       outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))       \n",
        "       predictions = self.fc(outputs)\n",
        "       predictions = predictions.squeeze(0)  # [1, batch_size, len_of_vocab] to [batch_size, len_of_vocab]\n",
        "      \n",
        "       return predictions, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder):\n",
        "       super().__init__()      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "\n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = source.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_size\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(device)\n",
        "\n",
        "       encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "       x = target[0]\n",
        "\n",
        "       for t in range(1, target_length):   \n",
        "           output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "           outputs[t] = output\n",
        "\n",
        "           best_guess = output.argmax(1)\n",
        "\n",
        "           x = target[t] if random.random() < teacher_forcing_ratio else best_guess\n",
        "\n",
        "       return outputs\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "def evaluate(model, input_text, output_text, sentences, criterion, max_length=MAX_LENGTH):\n",
        "  input_tensor = tensorFromSentence(input_text, sentences[0])\n",
        "  target_tensor = tensorFromSentence(output_text, sentences[1])\n",
        "  output = model(input_tensor, target_tensor)\n",
        "  num_iter = output.size(0)\n",
        "  loss = 0\n",
        "  decoded_words = []\n",
        "\n",
        "  for ot in range(num_iter):\n",
        "    loss += criterion(output[ot], target_tensor[ot])\n",
        "    topv, topi = output[ot].topk(1)\n",
        "    if topi[0].item() == EOS_token:\n",
        "      decoded_words.append('<EOS>')\n",
        "      break\n",
        "    else:\n",
        "      decoded_words.append(output_text.index2word[topi[0].item()])\n",
        "  return float(loss/num_iter), decoded_words\n",
        "\n",
        "def evaluateRandomly(model, source, target, pairs, criterion, n=10):\n",
        "  eval_loss = 0.0\n",
        "  for i in range(n):\n",
        "    pair = random.choice(pairs)\n",
        "    loss, decoded_words = evaluate(model, source, target, pair, criterion)\n",
        "    output_sentence = ' '.join(decoded_words)\n",
        "    eval_loss += loss\n",
        "    print('=====')\n",
        "    print('answer: {}'.format(pair[0]))\n",
        "    print('target question: {}'.format(pair[1]))\n",
        "    print('predicted question: {}'.format(output_sentence))\n",
        "    print('=====')\n",
        "  return float(eval_loss/n)\n",
        "\n",
        "def clacModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "   model_optimizer.zero_grad()\n",
        "\n",
        "   input_length = input_tensor.size(0)\n",
        "   loss = 0\n",
        "   epoch_loss = 0\n",
        "   \n",
        "   output = model(input_tensor, target_tensor)\n",
        "\n",
        "   num_iter = output.size(0)\n",
        "   \n",
        "   for ot in range(num_iter):\n",
        "       loss += criterion(output[ot], target_tensor[ot])\n",
        "\n",
        "   loss.backward()\n",
        "   model_optimizer.step()\n",
        "   epoch_loss = loss.item() / num_iter\n",
        "\n",
        "   return epoch_loss\n",
        "\n",
        "def trainModel(model, source, target, pairs, num_iteration):\n",
        "   model.train()\n",
        "\n",
        "   #optimizer = optim.SGD(model.parameters(), lr=3e-4)\n",
        "   optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "   criterion = nn.CrossEntropyLoss()\n",
        "   #criterion = nn.NLLLoss()\n",
        "   total_loss_iterations = 0\n",
        "\n",
        "   training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                     for i in range(num_iteration)]\n",
        "  \n",
        "   check_point = 1\n",
        "   for iter in tqdm(range(1, num_iteration+1)):\n",
        "       training_pair = training_pairs[iter - 1]\n",
        "       input_tensor = training_pair[0]\n",
        "       target_tensor = training_pair[1]\n",
        "\n",
        "       loss = clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "\n",
        "       total_loss_iterations += loss\n",
        "       \n",
        "       if iter % 50000 == 0:\n",
        "           avarage_loss= total_loss_iterations / 50000\n",
        "           total_loss_iterations = 0\n",
        "           print('------------------------')\n",
        "           print ('Check Point %d:' % check_point)\n",
        "           check_point = check_point + 1\n",
        "           print('Training Loss: %.4f' % (avarage_loss))\n",
        "          \n",
        "   return model\n",
        "\n",
        "src_file = 'answer'\n",
        "tgt_file = 'question'\n",
        "\n",
        "source, target, pairs = process_data(src_file, tgt_file)\n",
        "\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "\n",
        "input_size = source.n_words\n",
        "output_size = target.n_words\n",
        "\n",
        "embed_size = 100\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "num_iteration = 1000000\n",
        "dropout = 0.0\n",
        "\n",
        "encoder = Encoder(input_size, embed_size, hidden_size, num_layers, dropout)\n",
        "decoder = Decoder(output_size, embed_size, hidden_size, output_size, num_layers, dropout)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "print ('Model is initialized')\n",
        "\n",
        "print ('Training starts . . .')\n",
        "model = trainModel(model, source, target, pairs, num_iteration)\n",
        "\n",
        "\n",
        "print ('-----------Evaluation-------------------')\n",
        "model.eval()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "eval_loss = evaluateRandomly(model, source, target, pairs, criterion)\n",
        "print('Validation Loss: %.4f' % (eval_loss))\n",
        "print('------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Processing Data, the number of records: 264935\n",
            "File reading complete\n",
            "Processing Text . . . \n",
            "Processing Text . . . \n",
            "Processing Complete\n",
            "After Processing Data, the number of records: 72463\n",
            "Model is initialized\n",
            "Training starts . . .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c67ff5e62df4d1d880298ca5515c73c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------\n",
            "Check Point 1:\n",
            "Training Loss: 5.6293\n",
            "------------------------\n",
            "Check Point 2:\n",
            "Training Loss: 5.2806\n",
            "------------------------\n",
            "Check Point 3:\n",
            "Training Loss: 5.1545\n",
            "------------------------\n",
            "Check Point 4:\n",
            "Training Loss: 5.0307\n",
            "------------------------\n",
            "Check Point 5:\n",
            "Training Loss: 4.9080\n",
            "------------------------\n",
            "Check Point 6:\n",
            "Training Loss: 4.8033\n",
            "------------------------\n",
            "Check Point 7:\n",
            "Training Loss: 4.7423\n",
            "------------------------\n",
            "Check Point 8:\n",
            "Training Loss: 4.6575\n",
            "------------------------\n",
            "Check Point 9:\n",
            "Training Loss: 4.6020\n",
            "------------------------\n",
            "Check Point 10:\n",
            "Training Loss: 4.5363\n",
            "------------------------\n",
            "Check Point 11:\n",
            "Training Loss: 4.4807\n",
            "------------------------\n",
            "Check Point 12:\n",
            "Training Loss: 4.4434\n",
            "------------------------\n",
            "Check Point 13:\n",
            "Training Loss: 4.4046\n",
            "------------------------\n",
            "Check Point 14:\n",
            "Training Loss: 4.3774\n",
            "------------------------\n",
            "Check Point 15:\n",
            "Training Loss: 4.3293\n",
            "------------------------\n",
            "Check Point 16:\n",
            "Training Loss: 4.2953\n",
            "------------------------\n",
            "Check Point 17:\n",
            "Training Loss: 4.2626\n",
            "------------------------\n",
            "Check Point 18:\n",
            "Training Loss: 4.2156\n",
            "------------------------\n",
            "Check Point 19:\n",
            "Training Loss: 4.1959\n",
            "------------------------\n",
            "Check Point 20:\n",
            "Training Loss: 4.1439\n",
            "\n",
            "-----------Evaluation-------------------\n",
            "=====\n",
            "answer: not sure but it will not fit in the cup holder in my car\n",
            "target question: can you tell me the dimensions of the base please\n",
            "predicted question: SOS you fit me the the of the cup <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: theres a size chart on their site and where to measure yoyr knee\n",
            "target question: how do you measure for size\n",
            "predicted question: SOS do you measure from size <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: push the little lbkg button and that should switch it back and forth\n",
            "target question: how do i change from kg to pounds\n",
            "predicted question: SOS do i know the extension <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: the last time i purchased replacement heads amazon had them\n",
            "target question: i have a sonic electric toothbrush model  which i cant find replacement heads for any suggestions\n",
            "predicted question: SOS have shark sonic sonic shaver   what model is find the heads <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: if you dont mind the gap it would be difficult i think\n",
            "target question: can you pull two together closely to make a king size bed\n",
            "predicted question: SOS you install in the <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: battery but there is another emjoi product thats corded\n",
            "target question: is it battery power or corded\n",
            "predicted question: SOS it battery operated or rechargable <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: more like a faded black i think\n",
            "target question: what color is the stamped design  brown or green\n",
            "predicted question: SOS is is the finish brushed is brown or brown <EOS>\n",
            "=====\n",
            "=====\n",
            "answer:  amps and is the quietest we have ever owned\n",
            "target question: how many amps does this vacuum draw\n",
            "predicted question: SOS many amps does this unit draw <EOS>\n",
            "=====\n",
            "=====\n",
            "answer: from what you see yes but not the typically type more like the feel of melamine\n",
            "target question: the whole spigot is plastic\n",
            "predicted question: SOS is reading is is it\n",
            "=====\n",
            "=====\n",
            "answer: what is the  cfm stands for\n",
            "target question: cfm \n",
            "predicted question: SOS  <EOS>\n",
            "=====\n",
            "Validation Loss: 3.3537\n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}