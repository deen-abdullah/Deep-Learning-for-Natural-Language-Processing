{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Review.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabimist/Deep-Learning-for-Natural-Language-Processing/blob/main/Movie_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJW3UToOqPJo"
      },
      "source": [
        "**Name:** Deen Mohammad Abdullah\n",
        "\n",
        "**Student ID:** 001209782\n",
        "\n",
        "The purpose of this project is to train and test Movie Review dataset of NLTK using word embedding. Here, I have used CountVectorizer of sklearn to covert words into vector and Logistic Regression as the model.\n",
        "\n",
        "**Deep Learning for Natural Language Processing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJSnwafis2Ex"
      },
      "source": [
        "Execute the following code to run the project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2HBpi3CebsX",
        "outputId": "4aa4eed1-3acb-4067-c758-e1b1b616b9c9"
      },
      "source": [
        "####################### Importing Packages ######################\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('movie_reviews') # -------------------------------- downloading the movie_review dataset\n",
        "nltk.download('stopwords') # ------------------------------------ downloading the stopwords from NLTK\n",
        "nltk.download('wordnet')  # ------------------------------------- for lematization we need to download wordnet (WordNetLemmatizer)\n",
        "from nltk.corpus import movie_reviews # ------------------------- importing the movie_review dataset\n",
        "from nltk.corpus import stopwords # ----------------------------- importing the stopwords\n",
        "from nltk.stem import WordNetLemmatizer # ----------------------- importing wordNetLemmatizer for lemmatization\n",
        "import re # ----------------------------------------------------- importing regularExpression to extract only text from the dataset\n",
        "from sklearn import preprocessing  # ---------------------------- to process the movie categories into vectors as labels\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # -- to perform the wordToVector operation and create featuresMatrix\n",
        "from sklearn.model_selection import train_test_split # ---------- splitting the dataset into training and testing sets\n",
        "from sklearn.linear_model import LogisticRegression # ----------- the regression model to classify the reviews\n",
        "from sklearn import metrics # ----------------------------------- to print the accuracy, precision and recall scores of the model\n",
        "#################################################################\n",
        "\n",
        "#-------- This function remove special charecters from text -----------\n",
        "def removeSpecialCharacter(word_list):\n",
        "  cleanWordList = []\n",
        "  \n",
        "  for word in word_list:\n",
        "    if (re.match('[a-zA-Z]+', word)):\n",
        "      cleanWordList.append(word.lower())\n",
        "  \n",
        "  return cleanWordList\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "#--------- This function remove special characters from text -----------\n",
        "def removeStopWords (word_list):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "  filteredWords = [] \n",
        "  \n",
        "  for word in word_list:\n",
        "    if word not in stop_words: \n",
        "      filteredWords.append(word)\n",
        "      \n",
        "  return filteredWords\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "#----------This function uses WordNet and lematizes the text ------------\n",
        "def lemmatize (word_list):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  filteredWords = []\n",
        "  \n",
        "  for word in word_list:\n",
        "    filteredWords.append(lemmatizer.lemmatize(word))\n",
        "    \n",
        "  return filteredWords\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "############# The Executable Statements start from here ########################\n",
        "\n",
        "# taking all the reviews and their corresponding category into document\n",
        "document = [(movie_reviews.words(file_id),category) for file_id in movie_reviews.fileids() for category in movie_reviews.categories(file_id)] \n",
        "\n",
        "review = []\n",
        "categories = []\n",
        "\n",
        "# For each review:\n",
        "#     removing the special characters and stop words\n",
        "#     lemmatizing the text\n",
        "for (word_list,category) in document:\n",
        "  word_list = removeSpecialCharacter (word_list)\n",
        "  word_list = removeStopWords (word_list)\n",
        "  word_list = lemmatize (word_list)\n",
        "  \n",
        "  text = ''\n",
        "  for word in word_list:   \n",
        "    text = text + word + ' '\n",
        "  \n",
        "  review.append(text)\n",
        "  categories.append(category)\n",
        "\n",
        "# generating featureMatrix after performing word2Vec operation\n",
        "word2vec = CountVectorizer()\n",
        "featuresMatrix = word2vec.fit_transform(review)\n",
        "\n",
        "# processing the categories into labels vector\n",
        "leb = preprocessing.LabelEncoder()\n",
        "labels = leb.fit_transform(categories)\n",
        "\n",
        "print ('Feature Matrix Size: ' + str(featuresMatrix.shape))\n",
        "print ('Labels Size: ' + str(labels.shape))\n",
        "\n",
        "# splitting the dataset into training and testing sets (training 90% and testing 10%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(featuresMatrix, labels, test_size=0.10)\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(random_state = 0, solver = 'lbfgs', max_iter = 20000, multi_class = 'auto')\n",
        "model.fit (X_train, y_train)\n",
        "\n",
        "# Test the model\n",
        "result = model.predict (X_test)\n",
        "\n",
        "# Displaying the performance\n",
        "print ('Precision score of the model: ' + str(metrics.precision_score(y_test, result, average='macro', labels = np.unique (result))))\n",
        "print('Recall score of the model: ' + str(metrics.recall_score (y_test, result, average = 'macro',labels = np.unique(result))))\n",
        "print ('Accuracy of the model: ' + str(metrics.accuracy_score(y_test, result)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Feature Matrix Size: (2000, 34442)\n",
            "Labels Size: (2000,)\n",
            "Precision score of the model: 0.8243107769423559\n",
            "Recall score of the model: 0.824668807707748\n",
            "Accuracy of the model: 0.825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}