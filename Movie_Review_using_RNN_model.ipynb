{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Review using RNN model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BRDnibg682hTVEWXIytwxmnfONvtxjTi",
      "authorship_tag": "ABX9TyMWWgjlRoYDBhnCtbq+g85k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d2e544b130ca46e8bc1e8f522d6572fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_28aeb0a5326d4bbc9df9d9c97d68bd44",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_12eeccf61d354a60817519776d3e4f68",
              "IPY_MODEL_2873078b304b436789a299823216d97d"
            ]
          }
        },
        "28aeb0a5326d4bbc9df9d9c97d68bd44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12eeccf61d354a60817519776d3e4f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_537297c90f294b80af20e64e930e0960",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ed551ee44994df5bdc6c514b1cbe5d2"
          }
        },
        "2873078b304b436789a299823216d97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1d37b40e7ea4f1da7f061eccaa89cca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50000/50000 [02:16&lt;00:00, 365.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4664137676fd4b6ca2f8a7eae0bf371d"
          }
        },
        "537297c90f294b80af20e64e930e0960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ed551ee44994df5bdc6c514b1cbe5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1d37b40e7ea4f1da7f061eccaa89cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4664137676fd4b6ca2f8a7eae0bf371d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabimist/Deep-Learning-for-Natural-Language-Processing/blob/main/Movie_Review_using_RNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enK1z_od2hA"
      },
      "source": [
        "**Name:** Deen Mohammad Abdullah\n",
        "\n",
        "The purpose of this project is to train and test Movie Review dataset from IMDB using RNN (LSTM).\n",
        "\n",
        "**Deep Learning for Natural Language Processing**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "referenced_widgets": [
            "d2e544b130ca46e8bc1e8f522d6572fc",
            "28aeb0a5326d4bbc9df9d9c97d68bd44",
            "12eeccf61d354a60817519776d3e4f68",
            "2873078b304b436789a299823216d97d",
            "537297c90f294b80af20e64e930e0960",
            "4ed551ee44994df5bdc6c514b1cbe5d2",
            "c1d37b40e7ea4f1da7f061eccaa89cca",
            "4664137676fd4b6ca2f8a7eae0bf371d"
          ]
        },
        "id": "c25koo0pF0az",
        "outputId": "1fbc54bf-69af-4d62-f2a2-f1129e7d23a9"
      },
      "source": [
        "############################################ Required Packages #####################################################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords') # ------------------------------------ downloading the stopwords from NLTK\n",
        "nltk.download('wordnet')  # ------------------------------------- for lematization we need to download wordnet (WordNetLemmatizer)\n",
        "nltk.download('punkt') # ---------------------------------------- used for word tokenization\n",
        "from nltk.corpus import stopwords # ----------------------------- importing the stopwords\n",
        "from nltk.stem import WordNetLemmatizer # ----------------------- importing wordNetLemmatizer for lemmatization\n",
        "from nltk.tokenize import word_tokenize # ----------------------- tokenize words from text\n",
        "from nltk.corpus import stopwords #------------------------------ importing stop words \n",
        "import re # ----------------------------------------------------- importing regularExpression to extract only text from the dataset\n",
        "from sklearn.model_selection import train_test_split #----------- split the dataset for training and validation set\n",
        "import torch\n",
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "######################################################################################################################################\n",
        "\n",
        "########################################### Function Definitions (I have defined nine functions) #####################################\n",
        "#------ (1) This function removes special charecters from text ---------\n",
        "def removeSpecialCharacter(word_list):\n",
        "  cleanWordList = []\n",
        "  \n",
        "  for word in word_list:\n",
        "    if (re.match('[a-zA-Z0-9]+', word)):\n",
        "      cleanWordList.append(word.lower())\n",
        "  \n",
        "  return cleanWordList\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "#------ (2) This function removes stop words from text -----------------\n",
        "def removeStopWords (word_list):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "  filteredWords = [] \n",
        "  \n",
        "  for word in word_list:\n",
        "    if word not in stop_words: \n",
        "      filteredWords.append(word)\n",
        "      \n",
        "  return filteredWords\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "#------ (3) This function uses WordNet and lematizes the text -----------\n",
        "def lemmatize (word_list):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  filteredWords = []\n",
        "  \n",
        "  for word in word_list:\n",
        "    filteredWords.append(lemmatizer.lemmatize(word))\n",
        "    \n",
        "  return filteredWords\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "#--- (4) This function tokenize text, build vocabulary, preparing input  \n",
        "def extractFeature(document, category):\n",
        "  texts = []\n",
        "  print ('Processing data!!! Please wait...')\n",
        "  \n",
        "  for i in tqdm(range( len(document))):\n",
        "    word_list = removeSpecialCharacter (word_tokenize(document[i]))\n",
        "    word_list = removeStopWords (word_list)\n",
        "    word_list = lemmatize (word_list)\n",
        "    \n",
        "    txt = ''\n",
        "    for w in word_list:\n",
        "      txt = txt + w + ' '\n",
        "      \n",
        "    texts.append(txt)\n",
        "  \n",
        "  label = [1 if label =='positive' else 0 for label in category]\n",
        "  x_train, x_val, y_train, y_val = train_test_split(texts, label, test_size=0.2)\n",
        "  \n",
        "  word_list = []\n",
        "  for sent in x_train:\n",
        "    for word in sent.lower().split():\n",
        "      word_list.append(word)\n",
        "  \n",
        "  corpus = Counter(word_list)\n",
        "  corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
        "  vocab = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "  \n",
        "  final_list_train,final_list_test = [],[]\n",
        "  for sent in x_train:\n",
        "    final_list_train.append([vocab[word] for word in sent.lower().split() \n",
        "                                     if word in vocab.keys()])\n",
        "  for sent in x_val:\n",
        "    final_list_test.append([vocab[word] for word in sent.lower().split() \n",
        "                                    if word in vocab.keys()])\n",
        "  \n",
        "  return np.array(final_list_train), np.array(y_train), np.array(final_list_test), np.array(y_val), vocab\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "# ---- (5) Purpose of this function is to convert training ----------------------\n",
        "#---------- and validation dataset into torchTensor -----------------------------\n",
        "def data_loader(x_train, x_test, y_train, y_test):\n",
        "  seq_len = 500\n",
        "  \n",
        "  x_train_pad = np.zeros((len(x_train), seq_len),dtype=int)\n",
        "  for ii, review in enumerate(x_train):\n",
        "    if len(review) != 0:\n",
        "      x_train_pad[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "  \n",
        "  x_test_pad = np.zeros((len(x_test), seq_len),dtype=int)\n",
        "  for ii, review in enumerate(x_test):\n",
        "    if len(review) != 0:\n",
        "      x_test_pad[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "\n",
        "  train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
        "  valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
        "  batch_size = 50\n",
        "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "  return train_loader, valid_loader, batch_size\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "########### LSTM Model Architecture starts from here #############################\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,output_dim,drop_prob=0.5):\n",
        "        super(LSTM,self).__init__()\n",
        " \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        " \n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "    \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim, num_layers=no_layers, batch_first=True,  bidirectional=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "    \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "        # converting [batch_size, sequence_len, hidden_dim] to [batch_size*sequence_len, hidden_dim]\n",
        "        # to match the Linear layers expectation. Most of the data will be discarded later on after the sigmoid\n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        # get last batch of labels\n",
        "        sig_out = sig_out[:, -1] \n",
        "        # Shape if sig_out: [batch_size]\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "        \n",
        "                \n",
        "    def init_hidden(self, batch_size):\n",
        "        # hidden state\n",
        "        h0 = torch.zeros((self.no_layers*2,batch_size,self.hidden_dim)).to(device)\n",
        "        \n",
        "        # cell state\n",
        "        c0 = torch.zeros((self.no_layers*2,batch_size,self.hidden_dim)).to(device)\n",
        "\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "#---------------------------------------------------------------------------------\n",
        "criterion = nn.BCELoss()\n",
        "# -------- (6) This function is initializing the CNN model ----------------------------\n",
        "def initilize_model(vocab):\n",
        "  no_layers = 2\n",
        "  vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "  embedding_dim = 64\n",
        "  output_dim = 1\n",
        "  hidden_dim = 256\n",
        "  \n",
        "  model = LSTM(no_layers,vocab_size,hidden_dim,embedding_dim,output_dim,drop_prob=0.5)\n",
        "  model.to(device)\n",
        "  \n",
        "  lr=0.001  \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  \n",
        "  return model, optimizer\n",
        "#--------------------------------------------------------------------------------\n",
        "############ RNN Model architecture ends here #####################################\n",
        "\n",
        "###################### Training and Evaluation of Model ###########################\n",
        "#---- (7) this function evaluates the model -------------------------------------\n",
        "def evaluate(model, valid_loader, batch_size):\n",
        "  val_h = model.init_hidden(batch_size)\n",
        "  val_losses = []\n",
        "  val_acc = 0.0\n",
        "  model.eval()\n",
        "  for inputs, labels in valid_loader:\n",
        "    val_h = tuple([each.data for each in val_h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, val_h = model(inputs, val_h)\n",
        "    val_loss = criterion(output.squeeze(), labels.float())\n",
        "    val_losses.append(val_loss.item())\n",
        "    pred = torch.round(output.squeeze())\n",
        "    accuracy = torch.sum(pred == labels.squeeze()).item()\n",
        "    val_acc += accuracy\n",
        "  \n",
        "  return val_losses, val_acc\n",
        "\n",
        "#------ (8) this function train the model -------------------------------------- \n",
        "def train(model, optimizer, train_loader, valid_loader, batch_size, epochs):\n",
        "  clip = 5\n",
        "  \n",
        "  print ('\\n Training Starts...')\n",
        "  for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state \n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)   \n",
        "      h = tuple([each.data for each in h])\n",
        "      \n",
        "      model.zero_grad()\n",
        "      output,h = model(inputs,h)\n",
        "      \n",
        "      # calculate the loss and perform backprop\n",
        "      loss = criterion(output.squeeze(), labels.float())\n",
        "      loss.backward()\n",
        "      train_losses.append(loss.item())\n",
        "\n",
        "      pred = torch.round(output.squeeze())\n",
        "      accuracy = torch.sum(pred == labels.squeeze()).item()\n",
        "      \n",
        "      train_acc += accuracy\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      optimizer.step()\n",
        "    \n",
        "    val_losses, val_acc = evaluate(model, valid_loader, batch_size)\n",
        "    \n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "    val_loss = np.mean(val_losses)\n",
        "    train_accuracy = train_acc/len(train_loader.dataset)*100\n",
        "    val_accuracy = val_acc/len(valid_loader.dataset)*100\n",
        "       \n",
        "    print (\"Epoch: \" + str (epoch + 1))\n",
        "    print (\"Training Loss: \" + str (\"{:.5f}\".format(avg_train_loss)) + \", Validation Loss: \" + str (\"{:.5f}\".format(val_loss)))\n",
        "    print (\"Training Accuracy: \" + str (\"{:.2f}\".format(train_accuracy)) + \"%, Validation Accuracy: \" + str (\"{:.2f}\".format(val_accuracy))+ \"%\")\n",
        "    print (\"-------------------------------------------------------------------\")\n",
        "    \n",
        "###############            Main Function              #########################\n",
        "############### Executable statements start from here #########################\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "data = '/content/drive/MyDrive/IMDB Dataset.csv'\n",
        "df = pd.read_csv(data)\n",
        "df.head()\n",
        "\n",
        "document, category = df['review'].values,df['sentiment'].values\n",
        "\n",
        "x_train, y_train, x_test, y_test, vocab = extractFeature(document, category)\n",
        "train_loader, valid_loader, batch_size = data_loader (x_train, x_test, y_train, y_test)\n",
        "lstm, optimizer = initilize_model(vocab)\n",
        "train(lstm, optimizer, train_loader, valid_loader, batch_size, epochs=3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Processing data!!! Please wait...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2e544b130ca46e8bc1e8f522d6572fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Training Starts...\n",
            "Epoch: 1\n",
            "Training Loss: 0.52733, Validation Loss: 0.44731\n",
            "Training Accuracy: 73.89%, Validation Accuracy: 78.88%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 2\n",
            "Training Loss: 0.37944, Validation Loss: 0.35703\n",
            "Training Accuracy: 83.56%, Validation Accuracy: 84.51%\n",
            "-------------------------------------------------------------------\n",
            "Epoch: 3\n",
            "Training Loss: 0.33225, Validation Loss: 0.34810\n",
            "Training Accuracy: 86.00%, Validation Accuracy: 85.34%\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}